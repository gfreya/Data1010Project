<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>SVD(Singular Value Decomposition) and PCA(Principle Component Analysis) in Real-life Applications | My Data-Science Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="SVD(Singular Value Decomposition) and PCA(Principle Component Analysis) in Real-life Applications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Connections between SVD and PCA &amp; Real-life Applications." />
<meta property="og:description" content="Connections between SVD and PCA &amp; Real-life Applications." />
<link rel="canonical" href="https://gfreya.github.io/Data1010Project/jupyter/2020/11/13/post.html" />
<meta property="og:url" content="https://gfreya.github.io/Data1010Project/jupyter/2020/11/13/post.html" />
<meta property="og:site_name" content="My Data-Science Blog" />
<meta property="og:image" content="https://gfreya.github.io/Data1010Project/images/chart-preview.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-11-13T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"Connections between SVD and PCA &amp; Real-life Applications.","url":"https://gfreya.github.io/Data1010Project/jupyter/2020/11/13/post.html","@type":"BlogPosting","headline":"SVD(Singular Value Decomposition) and PCA(Principle Component Analysis) in Real-life Applications","dateModified":"2020-11-13T00:00:00-06:00","datePublished":"2020-11-13T00:00:00-06:00","image":"https://gfreya.github.io/Data1010Project/images/chart-preview.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://gfreya.github.io/Data1010Project/jupyter/2020/11/13/post.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Data1010Project/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://gfreya.github.io/Data1010Project/feed.xml" title="My Data-Science Blog" /><link rel="shortcut icon" type="image/x-icon" href="/Data1010Project/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Data1010Project/">My Data-Science Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Data1010Project/about/">About Me</a><a class="page-link" href="/Data1010Project/search/">Search</a><a class="page-link" href="/Data1010Project/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">SVD(Singular Value Decomposition) and PCA(Principle Component Analysis) in Real-life Applications</h1><p class="page-description">Connections between SVD and PCA & Real-life Applications.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-11-13T00:00:00-06:00" itemprop="datePublished">
        Nov 13, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      24 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Data1010Project/categories/#jupyter">jupyter</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/gfreya/Data1010Project/tree/master/_notebooks/2020-11-13-post.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/Data1010Project/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/gfreya/Data1010Project/master?filepath=_notebooks%2F2020-11-13-post.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Data1010Project/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/gfreya/Data1010Project/blob/master/_notebooks/2020-11-13-post.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/Data1010Project/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#Mathematical-Theory-of-SVD">Mathematical Theory of SVD </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Eigenvectors-and-Eigenvalues">Eigenvectors and Eigenvalues </a></li>
<li class="toc-entry toc-h3"><a href="#Definition-of-SVD">Definition of SVD </a></li>
<li class="toc-entry toc-h3"><a href="#Properties-of-SVD">Properties of SVD </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Mathematical-Theory-of-PCA">Mathematical Theory of PCA </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Derivation-and-Calculation-of-PCA">Derivation and Calculation of PCA </a></li>
<li class="toc-entry toc-h3"><a href="#Mathematical-Example-of-PCA">Mathematical Example of PCA </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Connections-between-SVD-and-PCA">Connections between SVD and PCA </a>
<ul>
<li class="toc-entry toc-h3"><a href="#Mathematical-Relations-behind-PCA-and-SVD">Mathematical Relations behind PCA and SVD </a></li>
<li class="toc-entry toc-h3"><a href="#Implementation-of-PCA-Based-on-SVD">Implementation of PCA Based on SVD </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Real-life-Applications-of-SVD-and-PCA">Real-life Applications of SVD and PCA </a>
<ul>
<li class="toc-entry toc-h3"><a href="#A-Simple-Matrix-Example">A Simple Matrix Example </a></li>
<li class="toc-entry toc-h3"><a href="#Iris-Dataset-Examples">Iris Dataset Examples </a></li>
<li class="toc-entry toc-h3"><a href="#Image-Processing-Examples">Image Processing Examples </a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#Summary">Summary </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-11-13-post.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In many fields of research and application, it is usually necessary to observe the data with multiple variables, and then analyze and find the rules after collecting a large amount of data. Multivariate large data sets will undoubtedly provide rich information for research and application, but also increase the workload of data collection to a certain extent. More importantly, in many cases, there may be correlation among many variables, which increases the complexity of problem analysis. If each index is analyzed separately, the analysis is often isolated, and the information in the data can not be fully utilized. Therefore, the blind reduction of indicators will lose a lot of useful information, resulting in wrong conclusions.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can have an intuitive example about the disaster gotten from high dimensional data. Different from text information, image information occupies a large capacity so that its storage and transmission will be greatly limited. Assume there is a still color image and its size is 600x800, which means it's 800 pixels long and 600 pixels wide and there are 480000 poxels on the screen. A pixel of a still color image needs three memory cells, so the single image needs $3*480000=1440000=1406.25$KB disk storage space. If the color image is transmitted at 24 frames per second, the amount of data per second is $24*1406.25$KB$\approx33$MB, then a 640MB CD-ROM can only store about 20 seconds of original data. With the present technology, it is difficult to meet the storage and transmission needs of the original digital image. We all know that image processing is an important task in the field of machine learning, so it is inevitable that we have to do something on the target images to compress the images and extract important features.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore, it is necessary to find a reasonable method to reduce the loss of information contained in the original indicators while reducing the indicators to be analyzed, so as to achieve the purpose of comprehensive analysis of the collected data. Since there is a certain correlation between the variables, we can consider changing the closely related variables into as few new variables as possible, so that these new variables are uncorrelated, then we can use less comprehensive indicators to represent the various kinds of information in each variable. Dimension reduction is a preprocessing method for high dimensional feature data. Dimension reduction is to preserve some of the most important features of high-dimensional data, remove noise and unimportant features, so as to improve the speed of data processing. In the actual production and application, dimension reduction can save us a lot of time and cost in a certain range of information loss. Dimension reduction has become a widely used data preprocessing method and the kind of reduction has several strong points:</p>
<ul>
<li>Make the data set feasible to explore and utilize</li>
<li>Decrease the number of feartures (only reserve the most important features)</li>
<li>Make sure the independence between features</li>
<li>Decrease the cost of algorithms</li>
<li>Make the results understandable</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So how to reduce dimensions like image compressing and features extraction is worth deep thinking. Thanks to the SVD(singular value decomposition which is a descent algebraic transformation, we can use the optimal matrix decompostion to do the image compression. Meanwhile, there is another similar method called PCA(principle component analysis) which is also used the matrix transformation and decomposition. The two methods have some connections and differences, they also have their own advantages. I want to introduction the mathematical conceptions behind SVD and PCA, explain the connections and difference between the two methods and compare the two methods via same task.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mathematical-Theory-of-SVD">
<a class="anchor" href="#Mathematical-Theory-of-SVD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mathematical Theory of SVD<a class="anchor-link" href="#Mathematical-Theory-of-SVD"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Eigenvectors-and-Eigenvalues">
<a class="anchor" href="#Eigenvectors-and-Eigenvalues" aria-hidden="true"><span class="octicon octicon-link"></span></a>Eigenvectors and Eigenvalues<a class="anchor-link" href="#Eigenvectors-and-Eigenvalues"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We first review the definitions of eigenvalues and eigenvectors as follows: $Ax = \lambda x$. In this formula, $A$ is an <em>n</em> $\times$ <em>n</em> real symmetric matrix, and $x$ is a vector of dimension n, then we say that $\lambda$ is an eigenvalue of matrix $A$, and $x$ is the eigenvector corresponding to the eigenvalue of matrix $A$. After we know the according eigenvalues and eigenvectors, we can do eigendecompositionn to the matrix $A$. Now we have n eigenvalues of matrix $A$: $\lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$ and the according eigenvectors {$v_1,v_2,\cdots,v_n$}, if the n eigenvectors are linearly independent, then the matrix can be eigendecomposed as $A = V\Lambda V^{-1}$.</p>
<p>$V$ is an <em>n</em>$\times$<em>n</em> matrix composed of the n eigenvectors and $\Lambda$ is the <em>n</em>$\times$<em>n</em> matrix with n eigenvalues on its main diagonal. Generaly, we will normalize the n eigenvectors in matrix $V$, namely satisefying $||v_i||_2 = 1$, or $v_i^T v_i = 1$. At this time, the eigenvectors of $V$ are the standard orthogonal bases, satisefying $V^TV=I$, namely $V^T = V^{-1}$. Then we can rewrite the eigendecomposition of matrix $A$: $A = V\Lambda V^T$.</p>
<p>To clarify one important point of eigendecomposition: the matrix $A$ must be a square matrix. So another question comes up: if the matrix $A$ is not a square matrix which means that the numbers of columns and rows are not the same, can we decompose the matrix $A$? The answer is definitely yes, and it just introduces the main topic SVD(singular value decomposition).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Definition-of-SVD">
<a class="anchor" href="#Definition-of-SVD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Definition of SVD<a class="anchor-link" href="#Definition-of-SVD"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>SVD is another method to decompose a matrix. Different from the eigen decomposition, SVD does not require the decomposed matrix to be the square one. Assume we have an <em>m</em>$\times$<em>n</em> matrix A, then we can define SVD of matrix $A$ as: $A = U\Sigma V^T$. The matrix $U$ is an <em>m</em>$\times$<em>m</em> matrix. The matrix $\Sigma$ is an <em>m</em>$\times$<em>n</em> matrix with singular values on the main diagonal. The matrix $V$ is an <em>n</em>$\times$<em>n</em> matrix. The matrix $U$ and $V$ satisefy $U^TU=I$ and $V^TV=I$. The following image can express the decomposition clearly.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/svd-def.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We all know that the matrix A here is not a square matrix, so we can do the matrix multiplication of the transpose of A and A. Then we can get an <em>n</em> x <em>n</em> matrix $A^TA$. Since the new created matrix $A^TA$ is a square one, we can do the eigen decomposition to them. Then the following formula can represent the process: $(A^TA)v_i = \lambda_iv_i$. According the formula, we can get the n eigenvalues and the n according eigenvectors $\boldsymbol{v}$. We can get the matrix $V$ using all eigenvectors of $A^TA$, the matrix $V$ is just the same $V$ in the formula of SVD. Generally, we call each eigenvector in matrix $V$ <em>the right singular vector</em> of $A$.</p>
<p>The same idea as above, but we will do all same step to $AA^T$. Then we can get $(AA^T)v_i = \lambda_iu_i$. Similarly, we call each eigenvector in matrix $U$ <em>the left singular vector</em> of $A$.</p>
<p>Now we can solve $U$ and $V$, the only problem is how to calculate the singular matrix $\Sigma$. We know that $\Sigma$ only has the singular values on the main diagonal and all other positions with value 0. So we only need to solve each singular value $\sigma$. We can know that:
$$
A = U\Sigma V^T \Rightarrow AV = U\Sigma V^TV \Rightarrow AV = U\Sigma \Rightarrow Av_i = \sigma_iu_i\Rightarrow \sigma_i = Av_i/u_i
$$</p>
<p>We can also verify that the eigenvectors of $A^TA$ and $AA^T$ make up the matrix $V$ and matrix $U$ in SVD using the following formula:
$$
A = U\Sigma V^T \Rightarrow A^T = V\Sigma^T U^T \Rightarrow A^TA = V\Sigma^T U^T U \Sigma V^T = V\Sigma^2V^T
$$
From the above formula, we can see that $U^TU=I$ and $\Sigma^T\Sigma = \Sigma^2$. Similarly, we can get that the eigenvectors matrix equals to the square of singular value vectors matrix which means we can know the relationship between the eigenvalues and singular values: $\sigma_i = \sqrt{\lambda_i}$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Properties-of-SVD">
<a class="anchor" href="#Properties-of-SVD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Properties of SVD<a class="anchor-link" href="#Properties-of-SVD"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Similar to the eigenvalues in eigen decomposition, the singular values arrange from larger to small in the singular values matrix, also the values decrease fast. That is to say, the sum of first 20% or even 1% of the whole singular values will account for more than 99% of the sum of all singular values generally. In another word, the useful property of SVD is that we can use the largest $k$ singular values and the corresponding sigular vectors to approximately represent the target matrix. The whole process can be represented as the following formula:
$$
A_{mxn} = U_{mxm}\Sigma_{mxn}V^T_{nxn}\approx U_{mxk}\Sigma_{kxk}V^T_{kxn}
$$
$k$ is much smaller than $n$, so the large target matrix $A$ can be represented by three small matrices $U_{mxk}$,$\Sigma_{kxk}$,$V^T_{kxn}$. We can use a diagram to represent the process.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/svd-property.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Mathematical-Theory-of-PCA">
<a class="anchor" href="#Mathematical-Theory-of-PCA" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mathematical Theory of PCA<a class="anchor-link" href="#Mathematical-Theory-of-PCA"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The meaning of PCA(principle component analysis is to find the most inportant part of data to substitute the raw data. PCA is one of the most important dimension reduction methods. In detail, assume we have an $n$ dimension dataset with m data $(x_1,x_2,\cdots,x_m)$. We want to reduce the dimensions of this data from $n$ dimensions to $k$ dimensions, and hope the $k$ dimensions data can best represent the raw data. The loss cannot be inevitable during the dimension reduction process, so we want to minimaize the loss.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see a very easy condition where $n$=2 and $k$=1. That is to say, the task is to reduce the dimensions of the data from 2 to 1. The data are shown as below.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/pca-2d-1d.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>According to the diagram above, we want to find one dimension to represent the 2-d data. We have two vectors listed in the diagram $\mu_1$ and $\mu_2$. Obviously, we will choose $\mu_1$. There are two main reasons. One is that the projections of the data on the line of $\mu_1$ direction will be best separate, the other is that the distances from the sample data to the line are enough small. If we want to make the $k$ from $1$ dimension to any dimensions, the criteria have the same thoughts: the projections of sample data on the hyperplane must be separatable enough and the distances between sample data and the hyperplane must be small enough.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Derivation-and-Calculation-of-PCA">
<a class="anchor" href="#Derivation-and-Calculation-of-PCA" aria-hidden="true"><span class="octicon octicon-link"></span></a>Derivation and Calculation of PCA<a class="anchor-link" href="#Derivation-and-Calculation-of-PCA"> </a>
</h3>
<blockquote>
<p>Based on Eigendecomposition</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Assume we have $m$ $n$-dimension data $(x_1, x_2, \cdots, x_m)$ with centralization, which means that the sum of the $m$ data is $0$. All $m$ data satisefy that $\sum_{i=1}^{m}{x_i}$=$0$. Also, we can assume that the new coordinate system with the bases after the projection transformation will be {$\boldsymbol {w_1, w_2, \cdots, w_n}$}, and $\boldsymbol w$ is the orthonormal basis. What we want to do in PCA is to reduce the dimensions from $n$ to $k$, which means that we need to drop some useless coordinates and create a new coordinate system {$\boldsymbol {w_1, w_2, \cdots, w_k}$}. Then one of the sample data $x_i$ will have the expression for its projection in this $k$-dimension coordinate system: $p_i$ = $(p_{i1}, p_{i2}, \cdots, p_{ik})^T$, and $p_{ij}$=$w_j^T x_i$ which means that the it is the coordinate of $x_i$ in the $j$th dimension of the original $n$-dimension coordinate system, namely $w_j$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For any sample data $x_i$, it has the projection $W^Tx_i$ and the projection variance $x_iWW^Tx_i$. We want to maximize the sum of the projection variance of all sample data, so we will maximize the trace of $\sum_{i=1}^{m}{W^Tx_ix_i^TW}$, the following formula represents the whole thing:</p>
<p>Objective function: ${argmax}$ $tr(W^TXX^TW)$</p>
<p>Constarint function: $ W^TW = I$</p>
<p>To solve the above optimization problem, we can introduce the Lagrange multiplier $\lambda$, so we can get the new Lagrange function:
$$
L(W) = tr(W^TXX^TW + \lambda(W^TW-I))
$$
Then we can do the derivative for argument $W$ and we can get that:
$$
XX^TW + \lambda W = 0 \Rightarrow XX^TW = -\lambda W
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Interestingly, we can find that the expression is very similar to the eigen decomposition, so we can do some conclusions about the matrix $XX^T$ and the meaning of $-\lambda$. We can say that $W$ is the matrix which consists of the $k$ eigenvectors of matrix $XX^T$ and $-\lambda$ is just the eigenvalue matrix with eigenvalues on the diagonal and $0$ on other positions. When we want to do the dimension reduction, we just need to find the corresponding eigenvectors of the $k$ biggest eigenvalues. The $k$ eigenvectors just construct the needed matrix $W$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you want to learn more about the process of PCA, please refer to</p>
<blockquote>
<p>wikipedia:<a href="https://en.wikipedia.org/wiki/Principal_component_analysis">https://en.wikipedia.org/wiki/Principal_component_analysis</a></p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mathematical-Example-of-PCA">
<a class="anchor" href="#Mathematical-Example-of-PCA" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mathematical Example of PCA<a class="anchor-link" href="#Mathematical-Example-of-PCA"> </a>
</h3>
<blockquote>
<p>Based on Eigendecomposition</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We want to give an easy example that can represent the process of PCA clearly and understandably.</p>
<p>Assume we have a matrix $X$ = $\left\lgroup\begin{matrix}-1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \cr -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1\end{matrix}\right\rgroup$, we want to use PCA to make the $2$-dimension data to $1$-dimension.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span> <span class="o">-</span><span class="mi">1</span> <span class="mi">0</span> <span class="mi">2</span> <span class="mi">0</span><span class="p">;</span> <span class="o">-</span><span class="mi">2</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">1</span> <span class="mi">1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2×5 Array{Int64,2}:
 -1  -1  0  2  0
 -2   0  0  1  1</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Make sure the target matrix is de-meaned.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">println</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">]))</span>
<span class="n">println</span><span class="p">(</span><span class="n">mean</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="o">:</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>0.0
0.0
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can find that the mean value of each line of the matrix $X$ is $0$, so we do not need to do any change.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Calculate the covariance matrix.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The covariance matrix is $\frac{1}{n}XX^T$. (the use of $\frac{1}{n}$ or $\frac{1}{n-1}$ does not have too much influence)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">cXXᵀ</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">length</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">:</span><span class="p">])</span> <span class="o">*</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="o">'</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2×2 Array{Float64,2}:
 1.2  0.8
 0.8  1.2</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The whole calculation of the covariance process is 
$$
cXX^T = \frac{1}{5} \left\lgroup\begin{matrix}-1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \cr -2 &amp; -0 &amp; 0 &amp; 1 &amp; 1\end{matrix}\right\rgroup \left\lgroup\begin{matrix}-1 &amp; -2 \cr -1 &amp; 0 \cr 0 &amp; 0 \cr 2 &amp; 1 \cr 0 &amp; 1 \end{matrix}\right\rgroup = \left\lgroup\begin{matrix}1.2 &amp; 0.8 \cr 0.8 &amp; 1.2\end{matrix}\right\rgroup
$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Calculate the eigenvalues and eigenvectors of the covariance matrix.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">eigvals</span><span class="p">(</span><span class="n">cXXᵀ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2-element Array{Float64,1}:
 0.4
 2.0</pre>
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-julia"><pre><span></span><span class="n">P</span> <span class="o">=</span> <span class="o">-</span><span class="n">eigvecs</span><span class="p">(</span><span class="n">cXXᵀ</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>2×2 Array{Float64,2}:
 -0.707107  0.707107
  0.707107  0.707107</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By calculation, we can get that the eigenvalues are $\lambda_1=2$ and $\lambda_2=0.4$. The corrsponding eigenvectors are $v_1$ = $\left\lgroup\begin{matrix}\frac{1}{\sqrt 2} \cr \frac{1}{\sqrt 2}\end{matrix}\right\rgroup$ and $v_2$ = $\left\lgroup\begin{matrix}-\frac{1}{\sqrt 2} \cr \frac{1}{\sqrt 2}\end{matrix}\right\rgroup$, then we can get the matrix $P$ =  $\left\lgroup\begin{matrix}\frac{1}{\sqrt 2} \frac{1}{\sqrt 2} \cr -\frac{1}{\sqrt 2} \frac{1}{\sqrt 2}\end{matrix}\right\rgroup$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Get the dimension reduction reuslt</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We need to get the $1$-dimention result, so we only get the eigenvector corresponding to the largest eigenvalue.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use <code>julia</code> to get the same result, and also we can have a very clear diagram to see the whole process.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/pca-example.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Connections-between-SVD-and-PCA">
<a class="anchor" href="#Connections-between-SVD-and-PCA" aria-hidden="true"><span class="octicon octicon-link"></span></a>Connections between SVD and PCA<a class="anchor-link" href="#Connections-between-SVD-and-PCA"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Generally speaking, if we want to use PCA to reduce dimensions, we want to find the $k$ eigenvectors corresponding to the $k$ largest eigenvalues of the covariance matrix $\frac{1}{n}X^TT$. Then we can use the projection matrix consisting of the $k$ eigenvectors as the projection hyperplane. From the process, we can know that it is inevitable that we always need to get the result of the covariance matrix $\frac{1}{n}X^TX$. But when the features of the sample data are very large, the calculation process will both time comsuming and space consuming.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can also use SVD to get the projection matrix using the $k$ eigenvectors. But the reason why I want to mention about the advantages of SVD is that some algorithms of SVD can still get the result of right sigular matrix $V$ without calculating the covariance matrix $\frac{1}{n}X^TX$. That is to say, our PCA algorithms do not need to do the eigen decomposition, instead, we can let SVD to complete this. This method is very effective and efficient when the sample data are very large.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Mathematical-Relations-behind-PCA-and-SVD">
<a class="anchor" href="#Mathematical-Relations-behind-PCA-and-SVD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mathematical Relations behind PCA and SVD<a class="anchor-link" href="#Mathematical-Relations-behind-PCA-and-SVD"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this part, we want to verify two conclusions:</p>
<ul>
<li>There are square relationships between singular values and eigen values</li>
<li>The results of singular value decomposition and eigenvalue decomposition of covariance matrix are consistent</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Firstly, we will verify the first conclusion. We want to get some interesting relationships about covariance matrix $\frac{1}{n}X^TX$, but we only focus on parts of the matrix($X^TX$) for convenience. According to the definition of sigular value decompsition: $X = U\Sigma V^T$, we can get that $$X^TX = V\Sigma U^TU\Sigma V^T = V\Sigma^2V^T = V\Sigma^2V^{-1}.$$ And we know that $X^TX$ is a symmetric positive semidefinite matrix, so it can be eigendecompsed as
$$
X^TX = V\Lambda V^{-1}.
$$
We can find that the two formula are consistent. And when the order of eigenvalues is assigned, we can easily find that $$\Lambda = \Sigma^2.$$</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Given an SVD of matrix $X$($X = U\Sigma V^T$), we know that 1)The columns of $V$(right singular vectors) are eigenvectors of $X^TX$; 2)The columns of $U$(left singular vectors) are eigenvectors of $XX^T$. If you do not know this, you can refer to <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition">https://en.wikipedia.org/wiki/Singular_value_decomposition#Relation_to_eigenvalue_decomposition</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bearing the above two in the mind, we can make SVD on $X^TX$. Given that $X^TX = U_2\Sigma_2V_2^T$, we can know that $U_2$ is just the eigenvectors of $X^TXX^TX$(we can write the new matrix as $(X^TX)(X^TX)^T$).Then we can get that:
$$
X^TXX^TX = U_2\Sigma_2V_2^T(U_2\Sigma_2V_2^T)^T = U_2\Sigma_2^2U_2^T
$$
And according to eigendecompsition, we can get that $X^TX = V_2\Lambda_2V_2^{-1}$, so for the new matrix, we can get that:
$$
X^TXX^TX = V_2\Lambda_2^2V_2^{-1}
$$
Easily, we can find that $U_2=V_2$ and $\Sigma_2^2 = \Lambda_2^2$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Based on the verification that the SVD has the same result as eigen decompsition of convariance matrix, we can do <strong>PCA based on SVD</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Implementation-of-PCA-Based-on-SVD">
<a class="anchor" href="#Implementation-of-PCA-Based-on-SVD" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation of PCA Based on SVD<a class="anchor-link" href="#Implementation-of-PCA-Based-on-SVD"> </a>
</h3>
<blockquote>
<p>Based on Sigular Value Decomposition</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The implementation process is not difficult.
<br>
Input: dataset $X$ = {$x_1$, $x_2$, $\cdots$, $x_n$}
<br>
Target: Reduce the dimensions from $n$ to $k$
<br>
Process:</p>
<ul>
<li>De-meaned</li>
<li>Calculate the covariance matrix</li>
<li>Calculate the eigenvectors and eigenvalues <strong>based on SVD</strong>
</li>
<li>Sort eigenvalues from large to small and choose the first $k$ largest eigenvalues and corresponding eigenvectors.</li>
<li>Make the dataset into the new $k$ dimensions space via linear transfromation</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is another interesting aspect deserves exploration. We can explore meanings of left singular matrix $U$ and right singular matrix $V$.
$$
X^l_{k\times n} = U^T_{k\times m}X_{m\times n}
$$
We can easily find that the new matrix $X^l$ condenses the rows from $m$ to $k$ compared to the raw matrix $X$. So we can get the conclusion that the left singular matrix can condense the rows number. For the matrix $V$, we can assume the same sample data matrix which is <em>m</em> $\times$ <em>n</em>. Then we find the right singular matrix $V$ using the $k$ eigenvectors of matrix $\frac{1}{n}XX^T$ from SVD, then we can a new matrix $X^r$:
$$
X^r_{m\times k} = X_{m\times n}V^T_{n\times k}
$$
In this case, the right singular matrix can reduce dimensions for columns. When we use SVD to decompose covariance matrix to realize PCA (<strong>PCA based on SVD</strong>), and we can get PCA dimension reduction in two directions (row and column).</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we want to use a diagram to illustrate the connections between SVD and PCA.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/relation.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Real-life-Applications-of-SVD-and-PCA">
<a class="anchor" href="#Real-life-Applications-of-SVD-and-PCA" aria-hidden="true"><span class="octicon octicon-link"></span></a>Real-life Applications of SVD and PCA<a class="anchor-link" href="#Real-life-Applications-of-SVD-and-PCA"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this part, We want to show applications of the SVD and PCA. The applications will include a mathematical example which calculates a simple matrix, iris dataset examples which use first three features for plotting a 3-d image and first two features for comparison, and vivid image processing examples. We hope you can have fun in this part.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="A-Simple-Matrix-Example">
<a class="anchor" href="#A-Simple-Matrix-Example" aria-hidden="true"><span class="octicon octicon-link"></span></a>A Simple Matrix Example<a class="anchor-link" href="#A-Simple-Matrix-Example"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The raw matrix is $\left\lgroup\begin{matrix}-2 &amp; -1 &amp; 0 &amp; 2 &amp; 1 \cr 2 &amp; 2 &amp; 0 &amp; -1 &amp; -1 \cr 2 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \end{matrix}\right\rgroup$. Then we will do the basic PCA(based on eigendecomposition) to this data and choose two most important features, and we can get the result reduced matrix. In the process, we can easily calculate the <em>covariance matrix</em> <code>cov_Mat</code>, then we just use the basic algorithm of PCA which is using the eigen decomposition to get the <em>eigenvalues</em> $\Lambda$ and <em>eigenvectors</em> $V$. We will <em>rearrange</em> the eigenvectors and eigenvalues. For eigenvalues, we will choose eigenvalues by component or rate, then we can get the corresponding eigenvectors. Lastly, we can do some data transfromation to get the output data. And we can see the results using this PCA based on eigendecomposition:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/mat_pcaeig.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we want to do PCA based on SVD on the same raw matrix. Thanks to Scikit-learn, we can just use the <code>PCA</code> function and choose the component=2 and then use <code>fit!</code> to fit the pca model. Lastly, we can get the output data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/mat_pcasvd.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The last method is direct SVD. We just use the <code>svd</code> function to get the right sigular vector matrix $U$, the left sigular vector matrix $V$ and the sigular value matrix $\Sigma$. Then we can choose how many factors we need to use in this decomposition. Lastly, we can transform data to get the output result.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/mat_svd.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the SVD and PCA methods generally have the same result for simple data, the three methods are all feasible.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Iris-Dataset-Examples">
<a class="anchor" href="#Iris-Dataset-Examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Iris Dataset Examples<a class="anchor-link" href="#Iris-Dataset-Examples"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this part, We will use the <code>iris</code> dataset as examples. We will use the same dataset to do two things. The first is that we can see the 3-d result(instead of just on a plane) vividly via choosing three features. The second is that we want to do the normal things: choose two features of iris dataset and use different methods to compare results.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Firstly, I want to use the <code>MultivariateStats</code> package in <code>julia</code> which contains <code>PCA</code> function to do the dimension reduction operation. I choose the first 3 principal components and then use <code>scatter</code> and <code>plot</code> functions in <code>Plots</code> package to show a 3-d diagram.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The <code>PCA</code> function in <code>MultivariateStats</code> package has two core algorithms: <code>pcacov</code> and <code>pcasvd</code>. From the names, we can see that one algorithm uses the eigendecomposition of covariance matrix and the other one just uses SVD. The two algorithms have different parameters, the <code>PCA</code> function will call different algorithms based on different parameters. <code>pcacov</code> function has param <code>C</code> for covariance matrix and param <code>mean</code> for the mean vector of original samples. <code>pcasvd</code> function has param <code>Z</code> for centralized samples and param <code>mean</code> for the mean vector of original samples. For detailed information about this package, you can refer to: <a href="https://multivariatestatsjl.readthedocs.io/en/latest/pca.html">https://multivariatestatsjl.readthedocs.io/en/latest/pca.html</a> .</p>
<p>And we can see the 3-d plot with first three principle features.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/iris_3d.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we want to choose the first two principle features and use PCA(based on SVD), PCA(based on eigendecomposition) and direct SVD methods. We can see the four plots vividly.</p>
<p>To make the algorithms diversity, we use the PCA <strong>based on SVD</strong> so that we can directly call the <code>PCA</code> function in <code>sklearn.decomposition</code>. For the PCA <strong>based on eigendecomposition</strong>, we need to caculate the <em>covariance matrix</em>. For direct SVD, we can use <code>svd</code> function to get the needed matrices $U$, $\Sigma$ and $V$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/PCA&amp;SVD_iris.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Image-Processing-Examples">
<a class="anchor" href="#Image-Processing-Examples" aria-hidden="true"><span class="octicon octicon-link"></span></a>Image Processing Examples<a class="anchor-link" href="#Image-Processing-Examples"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This part is very interesting. We first use the direct SVD method to an <code>.png</code> image file, and we will choose different features to see the results of image compression. Then as normal, we will use SVD and PCA to one image and see the results of different methods.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The following image is an image with 800$\times$512 pixels. We want to know how to work with color channels to explore image compression using the SVD. We can see that the details including the colors, outlines and so on are very clear in this raw image.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/dog.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We know that the raw image above is 800$\times$512, which means that the raw image has at most 512 sigular values. Then we want to choose 10, 50 ,100 singular values respectively and get the according results compared with the raw image. We use the <code>map</code> function in <code>julia</code> language to choose different sigular values and use <code>mosaicview</code> function to show the results including the raw image in the first place. The result is shown as below:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/res_dog.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is not difficult to find that the compressed image is very fuzzy when choosing only 10 singular values. The outlines and the patterns of the lovely dog cannot be dealt with clearly, which means that the first 10 singular values cannot closely represent the whole information of the raw image. When we choose 50 singular images, we can see that the ouline of the dog is only a little fuzzy. And when we choose 100 singular values, we find it is very close to the raw image. That is to say, we can only use 100 singular values instead of 512 total to represent to dog image which means that less space to store the needed information and less time to deal with the related information. Without doubt, SVD is very useful and successful in image compression.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then we will do the same step as the previous examples. We want to compare the results of SVD and PCA to the same image. The <code>cat.png</code> image file is a cat with a mask, it is 1200$\times$1200 pixels. We use direct SVD method and PCA <strong>based on eigendecompostion</strong> method to compress the raw image. To make the problem simple, we transfer the <code>RGB</code> image to <code>Gray</code> image.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/cat.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the two images on the first row are the raw image and the corresponding gray image. Then we just get the matrices $U$, $V$ and $\Sigma$ via the embedded function <code>svd()</code>. When it comes to PCA methods, we use the covariance matrix and calculate the eigenvalues and eigenvectors. We can see that both compressed images represetnt the raw gray image very well.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/image-compre.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To explore deeper about the difference of SVD and PCA, we write another function to calculate how many corresponding values(singular values in SVD, eigenvalues in PCA) can represent 90% cumulative contribution rate for the raw gray image in different methods. We need 384 eigenvalues in PCA while we only need 362 singular values in SVD. So it is not difficult to summarize that the SVD method will do better on image compression on the same level.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/Data1010Project/images/copied_from_nb/images/im_comp_1.png" alt=""></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">
<a class="anchor" href="#Summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary<a class="anchor-link" href="#Summary"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This blog clarifies the methematical theories behind SVD and PCA using some formula. We just give some simple derivation process like how to caculate the singular matrix and singlar values in SVD and how to get the eigenvalues and eigenvectors of covariance matrix in PCA. If you want to explore deeper and wider, you can click on the links, I think they are very useful.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A very important topic in this blog is that there are two main algorithms of PCA. One is PCA based on eigendecomposition which uses covariance matrix and the other is PCA based on SVD which uses singular values and singular vectors. Because the basic PCA based on eigendecomposition cannot do as well as the PCA based on SVD in general, so some embedded <code>PCA</code> functions like <code>PCA</code> in <code>MultivariateStats</code> and <code>PCA</code> in <code>sklearn.decomposition</code> both use PCA <strong>based on SVD</strong> which is verified more effective and efficient.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The most interesting part of this blog is the vivid examples. We illustrate examples using simple matrix, iris dataset and some images. We hope you can find the connections and differences between different methods. Hope you can understand easily with the help of these diagrams.</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="gfreya/Data1010Project"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/Data1010Project/jupyter/2020/11/13/post.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Data1010Project/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Data1010Project/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Data1010Project/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/Data1010Project/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/Data1010Project/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
